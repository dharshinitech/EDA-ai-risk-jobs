ğŸ“Š Task 2 â€“ Exploratory Data Analysis (EDA)

ğŸ“Œ Project Overview

This project is part of my internship Task 2, where I performed Exploratory Data Analysis (EDA) on an AI Risk Jobs dataset. The main goal of this task is to understand the dataset structure, identify patterns, check data quality, and extract meaningful insights before applying any advanced analysis or modeling.

ğŸ“‚ Dataset Description

Dataset Name: AI Risk Jobs Dataset

Number of Records: 10 job roles

Key Columns:

Job Role

Salary

AI Risk Score

The dataset contains both categorical and numerical variables related to job roles and their risk of automation due to AI.

ğŸ¯ Objectives of EDA

Understand the structure and size of the dataset

Identify data types (categorical & numerical)

Check for missing values

Analyze salary distribution

Analyze AI risk scores across job roles

Gain insights for further visualization and analysis

ğŸ›  Tools & Libraries Used

Python

Google Colab

Pandas

NumPy

Matplotlib

Seaborn
ğŸ” Steps Performed in EDA

1ï¸âƒ£ Data Loading

Loaded the dataset into Google Colab using Pandas

Displayed the first few rows to understand the structure

2ï¸âƒ£ Data Inspection

Checked dataset shape (rows & columns)

Used info() to identify data types

Used describe() for statistical summary

3ï¸âƒ£ Missing Value Analysis

Verified that there are no missing values in the dataset

Confirmed good data quality

4ï¸âƒ£ Salary Analysis

Analyzed salary distribution across job roles

Observed that jobs with higher salaries tend to have lower AI risk

5ï¸âƒ£ AI Risk Analysis

Examined AI risk scores for each job role

Identified which jobs are more vulnerable to AI automation

ğŸ“ˆ Key Insights

The dataset contains clean and well-structured data

Jobs with higher salaries generally show lower AI risk

Certain job roles are more exposed to AI automation

The dataset is suitable for further data visualization and analysis

âœ… Conclusion

The Exploratory Data Analysis helped in understanding the dataset clearly. Since the data is clean and meaningful, it can be effectively used for data visualization (Task 3) and further analytical tasks.


